<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners -->
  <meta name="description" content="CellCut: Unsupervised Cellular Image Segmentation through Self-Supervision">
  <meta property="og:title" content="CellCut: Unsupervised Cellular Image Segmentation"/>
  <meta property="og:description" content="A novel unsupervised/semi-supervised method for cell image segmentation using self-supervised learning"/>
  <meta property="og:url" content="YOUR_WEBSITE_URL"/>
  <meta property="og:image" content="static/images/your_banner_image.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="CellCut: Unsupervised Cell Segmentation">
  <meta name="twitter:description" content="Self-supervised learning for cell image segmentation without manual annotations">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">

  <meta name="keywords" content="cell segmentation, computer vision, unsupervised learning, self-supervision, microscopy">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>CellCut</title>

  <link rel="icon" type="image/png" href="static/images/icon.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* Custom Styles for Headers */
    .section .title {
      font-weight: bold;
      font-size: 3rem; /* Make headers larger */
    }

    .section .title.is-3 {
      font-size: 3.5rem; /* Increase font size for Abstract, Methods, and Results */
      white-space: nowrap; /* Prevent header from wrapping */
    }

    .section .title.is-3, .section .title.is-2 {
      font-weight: 700; /* Make titles bolder */
    }

    /* Ensure the container does not force the header to wrap */
    .container.is-max-desktop {
      max-width: 60%; /* Allow the container to use the full available width */
    }


    #image-container {
      position: relative;
      display: flex; /* Use flexbox to center */
      justify-content: center; /* Center horizontally */
      align-items: center; /* Center vertically */
      width: 100%; /* Ensure it takes up full available width */
      height: auto; /* Adjust height based on image aspect ratio */
      margin: 0 auto; /* Center it in the page */
    }

    #main-image {
      max-width: 60%; /* Ensure the image scales within the container */
      height: auto; /* Maintain aspect ratio */
      display: block; /* Remove default inline behavior */
    }



  </style>



</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CellCut: Unsupervised Cellular Image Segmentation through Self-Supervision</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Markus Markus</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Aditya Mehta</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Andrew Zabelo</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Neehar Kondapaneni</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Pietro Perona</a>
                </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">California Institute of Technology<br>CVPR 2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/PLACEHOLDER" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/YOUR_REPO" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <!-- Google Colab link -->
              <span class="link-block">
                <a href="https://colab.research.google.com/your_notebook_link" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-google"></i>
                </span>
                <span>Colab</span>
              </a>
            </span>

            <!-- Arxiv link -->
            <!--     <span class="link-block">
                  <a href="https://arxiv.org/placeholder" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>ArXiv</span>
                </a>
              </span> -->

       <!-- Arxiv Link with icon from Academicons -->
<span class="link-block">
  <a href="https://arxiv.org/abs/2310.00031" target="_blank"
     class="external-link button is-normal is-rounded is-dark">
    <span class="icon">
      <i class="ai ai-arxiv"></i> <!-- Arxiv Icon from Academicons -->
    </span>
    <span>arXiv</span>
  </a>
</span>




          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- New GIF added here -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <figure class="image">
            <img src="static/images/figure1.gif" alt="CellCut Method Overview">
            <p class="content has-text-justified" style="margin-top: 30px;">
              CellCut enables the unsupervised/semi-supervised creation of high-quality pseudo masks. These pseudo masks are used to fine-tune a pretrained segmentation model, producing better masks than SOTA zero-shot methods.
            </p>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Abstract Section -->
<section class="section hero" style="background-color: #f0f0f0; margin-top: 80px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Abstract</h2> <!-- Header now larger and bolder -->
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
      		Cells are essential components of biological systems, and accurately segmenting them is crucial for analyzing microscopy images. While deep learning has improved cell segmentation, <b>it relies on large annotated datasets, which are often unavailable for new imaging conditions.</b> Variations in cell size, shape, and imaging modality usually require new training data, and existing models struggle with out-of-distribution cells. To address this, we propose a <b>novel approach leveraging self-supervised models</b> to identify patch-level similarities in cell images, which can be effectively applied in both unsupervised and semi-supervised settings. Leveraging these patch-level similarities, <b>our approach iteratively generates precise cell masks</b> to construct an initial training set, progressively refining segmentation accuracy via self-training. <b>This approach reduces the need for manual annotations</b>, enabling more effective high-throughput image analysis and broader applicability of segmentation models. We show that we can fine-tune state-of-the-art cell segmentation models on some out-of- distribution datasets and <b>achieve close to fully supervised fine-tuning performance with only 20 manually annotated cells (less than 0.1% of data)</b>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Methods Section -->
<section class="section hero" style="background-color: #f0f0f0;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Methods</h2> <!-- Header now larger and bolder -->
    </div>
  </div>
</section>



<section style="margin-top: 80px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/complete_method.png" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-justified" style="margin-top: 40px;">
            <div class="columns is-centered">
            <div class="column is-four-fifths">
            <br><b>CellCut: unsupervised cellular image segmentation.</b> Our method for training a zero/few-shot cellular segmentation model comprises several stages. First, we fine-tune a DINO model on an unlabeled cell image dataset. After training, DINO features are used with our region-growing method to generate coarse pseudo-masks. Coarse pseudo-masks are refined through self-training with DropLoss, resulting in high-quality masks. These refined masks are then used to fine-tune a cell segmentation model, achieving robust segmentation on out-of-sample datasets.
          </div>
        </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small" style="margin-top: 80px;">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <div class="container">
      <div class="columns is-centered">
        
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/masking_method.png" alt="Figure Missing">
            <figcaption class="content has-text-justified">
              <br><b>Pseudo-mask Generation.</b> First, we embed an image using a pre-trained DINO model. We select a seed feature (either supervised or unsupervised) and generate a similarity map between the seed feature and all other patches. We smooth the similarity map and perform a region-growing cut starting from the seed feature. The cut region is masked from the DINO features. This process is repeated until our heuristic stopping condition is reached.
            </figcaption>
          </figure>
        </div>

        <!-- Second image -->
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/seed_selection.png" alt="Figure Missing">
            <figcaption class="content has-text-justified">
              <br><br><b>Seed Selection.</b>  Our unsupervised method for seed selection uses the 2nd smallest eigenvector of the affinity matrix to select the maximally activated patch. Our semi-supervised method leverages the embeddings from only a few labeled cells and labeled background patches (∼ 20 each, which is < 0.1% of available labels) to produce a query “cell” feature to discover cells in the DINO latent space.
            </figcaption>
          </figure>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>

<!-- detector training -->

<section style="margin-top: 80px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/Roundwise.pdf" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-justified" style="margin-top: 40px;">
            <div class="columns is-centered">
            <div class="column is-four-fifths">
            <br><b>Qualitative assessment of method stages. </b> DropLoss and self-training refine coarse masks generated by our region- growing pseudo-mask generator for DSB and CIL.
          </div>
        </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
 </section>


<section class="section hero" style="background-color: #f0f0f0; margin-top: 80px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Results</h2> <!-- Header now larger and bolder -->
    </div>
  </div>
</section>



<!-- DSB -->
<section style="margin-top: 10px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/DSB_Qual_Big.pdf" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-centered" style="margin-top: 10px;">
            <div class="columns is-centered">
              <div class="column is-12 has-text-centered">
                <br><b>(a) DSB</b>
              </div>
            </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- CIL -->
<section style="margin-top: 10px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/Cellpose_Qual_Big.pdf" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-centered" style="margin-top: 10px;">
            <div class="columns is-centered">
              <div class="column is-12 has-text-centered">
                <br><b>(b) Cell Image Library</b>
              </div>
            </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>



<!-- Pannuke -->

<section style="margin-top: 10px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/diagrams.png" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-centered" style="margin-top: 10px;">
            <div class="columns is-centered">
              <div class="column is-12 has-text-centered">
                <br><b>(c) Pannuke</b>
              </div>
            </div>
          </figcaption>
          <figcaption class="content has-text-justified" style="margin-top: 10;">
            <div class="columns is-centered">
            <div class="column is-four-fifths">
            <br><b>Qualitative Results.</b> Despite having no/few labels, CellCut can effectively discriminate between instance and background in highly homogeneous images and generalizes to various cell types, sizes, and sparsities. The above displays prediction and ground truth masks on the (a) DSB, (b) Cell Image Library, and (c) Pannuke datasets. 
          </div>
        </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<!-- additional experiments -->

<section class="hero is-small" style="margin-top: 80px;">
  <div class="hero-body">
    <div class="container is-max-desktop">
    <div class="container">
      <div class="columns is-centered">
        
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/backbone_resolution_comparison.png" alt="Figure Missing">
            <figcaption class="content has-text-justified">
              <br><b>Higher Resolutions during Region Growing.</b>  We perform an ablation using varying crop sizes to examine the effects of different resolutions during the region growing stage for DSB unsupervised (we test crop sizes of 64 × 64, 128 × 128, and 256 × 256 pixels). We do this with the DINO and ViTDeT backbones before proceeding to the DropLoss and self-training stages. Higher resolutions consistently lead to better performance. Given this, we attempt to push the unsupervised performance on DSB by using the ViTDeT backbone with 64 × 64 crop size, finding that this yields significant improvement, further shrinking the gap to fully supervised methods.
            </figcaption>
          </figure>
        </div>

        <!-- Second image -->
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/models_ap50_bar_plot-4.png" alt="Figure Missing">
            <figcaption class="content has-text-justified">
              <br><br><b>Alternative Backbones.</b> To explore the behavior of our method when using different backbones, we run region-growing for DSB using hierarchical DINO trained on cellular data (HIPT), MAE, vanilla ViT, SAM-ViT, and ViTDeT. We find that out-of-the-box SAM-ViT and ViTDeT both perform better than DINO. This is somewhat expected, as SAM-ViT and ViTDeT are fine-tuned for segmentation/object detection on large-scale datasets.
            </figcaption>
          </figure>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero" style="background-color: #f0f0f0; margin-top: 80px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Fine-Tuning Results</h2> <!-- Header now larger and bolder -->
    </div>
  </div>
</section>



<!-- finetuning -->

<section style="margin-top: 10px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/ours cyto3.pdf" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-centered" style="margin-top: 10px;">
            <div class="columns is-centered">
              <div class="column is-12 has-text-centered">
                <br><b>(a) Our Fine-Tuning</b>
              </div>
            </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section style="margin-top: 10px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/cyto3 zero.pdf" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-centered" style="margin-top: 10px;">
            <div class="columns is-centered">
              <div class="column is-12 has-text-centered">
                <br><b>(b) Zero-Shot</b>
              </div>
            </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section style="margin-top: 10px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/cyto 3 few.pdf" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-centered" style="margin-top: 10px;">
            <div class="columns is-centered">
              <div class="column is-12 has-text-centered">
                <br><b>(c) Few-Shot</b>
              </div>
            </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section style="margin-top: 10px; width: 100%; max-width: 60%; margin-left: auto; margin-right: auto;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <figure class="image">
          <img src="static/images/cyto3 full.pdf" alt="Figure Missing" style="width: 100%; height: auto;">
          <figcaption class="content has-text-centered" style="margin-top: 10px;">
            <div class="columns is-centered">
              <div class="column is-12 has-text-centered">
                <br><b>(d) Fully Supervised</b>
              </div>
            </div>
          </figcaption>
          <figcaption class="content has-text-justified" style="margin-top: 10px;">
            <div class="columns is-centered">
            <div class="column is-four-fifths">
            <br><b>Fine-tuning of Pre-trained SOTA Model.</b>  As the final step in our pipeline, we fine-tune a pre-trained SOTA cell segmentation on pseudo labels generated by our pipeline to create a general cell segmentation model. To achieve this, we fine-tune the Cellpose model on our semi-supervised Pannuke labels. Our baseline is the zero-shot performance of Cellpose on Pannuke. As the upper baseline, we train Cellpose on the full Pannuke training dataset (fully supervised). The above displays prediction and ground truth masks split by cell type using (a) our method, (b) zero-shot, (c) few-shot, and (d) fully supervised approaches with the cyto3 pre-trained model.
          </div>
        </div>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>





<!-- BibTeX citation -->
<section class="section" id="BibTeX" style="margin-top: 60px;">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{cellcut2025,
  title={CellCut: Unsupervised Cellular Image Segmentation through Self-Supervision},
  author={Marks, Markus and Mehta, Aditya and and Zabelo, Andrew and Kondapaneni, Neehar and Perona, Pietro.},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (ICCV)},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            ICCV 2025 submission (Paper ID 6675). <br>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
</body>
</html>




